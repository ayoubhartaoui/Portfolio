{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Scraping: Verification of Image Existence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# GBIF API endpoint for occurrence search\n",
    "url = \"https://api.gbif.org/v1/occurrence/search\"\n",
    "params = {\n",
    "    \"scientificName\": \"Amphiprion ocellaris\",\n",
    "    \"mediaType\": \"StillImage\",\n",
    "    \"limit\": 300,  # Set to maximum allowed per request (300)\n",
    "}\n",
    "\n",
    "image_urls = []\n",
    "offset = 0\n",
    "while True:\n",
    "    # Update offset for each page\n",
    "    params[\"offset\"] = offset\n",
    "    response = requests.get(url, params=params)\n",
    "    data = response.json()\n",
    "    results = data.get(\"results\", [])\n",
    "\n",
    "    # Break if no more results\n",
    "    if not results:\n",
    "        break\n",
    "\n",
    "    # Extract image URLs\n",
    "    for record in results:\n",
    "        media_entries = record.get(\"media\", [])\n",
    "        for media in media_entries:\n",
    "            if media.get(\"type\") == \"StillImage\":\n",
    "                image_url = media.get(\"identifier\")\n",
    "                if image_url:\n",
    "                    image_urls.append(image_url)\n",
    "\n",
    "    # Update offset for the next page\n",
    "    offset += params[\"limit\"]\n",
    "\n",
    "print(f\"Total images found: {len(image_urls)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapling of target images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 500 images.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "# Set the species name and output directory\n",
    "species_name = \"Amphiprion ocellaris\"\n",
    "output_dir = \"dataset/Amphiprion_ocellaris_images\"  # Directory to save images\n",
    "gbif_api_url = \"https://api.gbif.org/v1/occurrence/search\"\n",
    "max_images = 500 # Limit the number of images for testing\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def download_images(species, output_folder, max_images):\n",
    "    # Set up API parameters\n",
    "    params = {\n",
    "        \"scientificName\": species,\n",
    "        \"mediaType\": \"StillImage\",\n",
    "        \"limit\": 300,  # GBIF API page limit is 300\n",
    "        \"offset\": 0\n",
    "    }\n",
    "    \n",
    "    downloaded_count = 0\n",
    "\n",
    "    # Loop until there are no more images left to download\n",
    "    while downloaded_count < max_images:\n",
    "        # Make the API request\n",
    "        response = requests.get(gbif_api_url, params=params)\n",
    "        data = response.json()\n",
    "\n",
    "        # Check if there are occurrences left to process\n",
    "        if not data['results']:\n",
    "            print(\"No more results found.\")\n",
    "            break\n",
    "\n",
    "        # Download each image found in the response\n",
    "        for result in data['results']:\n",
    "            if \"media\" in result:\n",
    "                for media in result['media']:\n",
    "                    if 'identifier' in media:\n",
    "                        image_url = media['identifier']\n",
    "                        try:\n",
    "                            # Download and save the image\n",
    "                            img_data = requests.get(image_url, timeout=10).content\n",
    "                            with open(f\"{output_folder}/img_{downloaded_count}.jpg\", \"wb\") as handler:\n",
    "                                handler.write(img_data)\n",
    "                            downloaded_count += 1\n",
    "                            \n",
    "                            # Check if the limit is reached\n",
    "                            if downloaded_count >= max_images:\n",
    "                                print(f\"Downloaded {downloaded_count} images.\")\n",
    "                                return\n",
    "\n",
    "                        except Exception as e:\n",
    "                            print(f\"Could not download {image_url}: {e}\")\n",
    "                            continue\n",
    "\n",
    "        # Update offset to fetch the next page of results\n",
    "        params[\"offset\"] += params[\"limit\"]\n",
    "\n",
    "    print(f\"Total images downloaded: {downloaded_count}\")\n",
    "\n",
    "# Run the image download function\n",
    "download_images(species_name, output_dir, max_images)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapling of non-target images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 300 images.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "# Set the species name and output directory\n",
    "species_name = \"Amphiprion clarkii\"\n",
    "output_dir = \"dataset/Amphiprion_clarkii_images\"  # Directory to save images\n",
    "gbif_api_url = \"https://api.gbif.org/v1/occurrence/search\"\n",
    "max_images = 300  # Limit the number of images for testing\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def download_images(species, output_folder, max_images):\n",
    "    # Set up API parameters\n",
    "    params = {\n",
    "        \"scientificName\": species,\n",
    "        \"mediaType\": \"StillImage\",\n",
    "        \"limit\": 300,  # GBIF API page limit is 300\n",
    "        \"offset\": 0\n",
    "    }\n",
    "    \n",
    "    downloaded_count = 0\n",
    "\n",
    "    # Loop until there are no more images left to download\n",
    "    while downloaded_count < max_images:\n",
    "        # Make the API request\n",
    "        response = requests.get(gbif_api_url, params=params)\n",
    "        data = response.json()\n",
    "\n",
    "        # Check if there are occurrences left to process\n",
    "        if not data['results']:\n",
    "            print(\"No more results found.\")\n",
    "            break\n",
    "\n",
    "        # Download each image found in the response\n",
    "        for result in data['results']:\n",
    "            if \"media\" in result:\n",
    "                for media in result['media']:\n",
    "                    if 'identifier' in media:\n",
    "                        image_url = media['identifier']\n",
    "                        try:\n",
    "                            # Download and save the image\n",
    "                            img_data = requests.get(image_url, timeout=10).content\n",
    "                            with open(f\"{output_folder}/img_{downloaded_count}.jpg\", \"wb\") as handler:\n",
    "                                handler.write(img_data)\n",
    "                            downloaded_count += 1\n",
    "                            \n",
    "                            # Check if the limit is reached\n",
    "                            if downloaded_count >= max_images:\n",
    "                                print(f\"Downloaded {downloaded_count} images.\")\n",
    "                                return\n",
    "\n",
    "                        except Exception as e:\n",
    "                            print(f\"Could not download {image_url}: {e}\")\n",
    "                            continue\n",
    "\n",
    "        # Update offset to fetch the next page of results\n",
    "        params[\"offset\"] += params[\"limit\"]\n",
    "\n",
    "    print(f\"Total images downloaded: {downloaded_count}\")\n",
    "\n",
    "# Run the image download function\n",
    "download_images(species_name, output_dir, max_images)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Not recommanded : Scrapling of all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set the species name and output directory\n",
    "species_name = \"Amphiprion ocellaris\"\n",
    "output_dir = \"dataset/Amphiprion_ocellaris_images\"  # Directory to save images\n",
    "gbif_api_url = \"https://api.gbif.org/v1/occurrence/search\"\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def download_images(species, output_folder):\n",
    "    # Set up API parameters\n",
    "    params = {\n",
    "        \"scientificName\": species,\n",
    "        \"mediaType\": \"StillImage\",\n",
    "        \"limit\": 300,  # GBIF API page limit is 300\n",
    "        \"offset\": 0\n",
    "    }\n",
    "    \n",
    "    downloaded_count = 0\n",
    "\n",
    "    # Loop until there are no more images left to download\n",
    "    while True:\n",
    "        # Make the API request\n",
    "        response = requests.get(gbif_api_url, params=params)\n",
    "        data = response.json()\n",
    "\n",
    "        # Check if there are occurrences left to process\n",
    "        if not data['results']:\n",
    "            print(\"No more results found.\")\n",
    "            break\n",
    "\n",
    "        # Download each image found in the response\n",
    "        for result in data['results']:\n",
    "            if \"media\" in result:\n",
    "                for media in result['media']:\n",
    "                    if 'identifier' in media:\n",
    "                        image_url = media['identifier']\n",
    "                        try:\n",
    "                            # Download and save the image\n",
    "                            img_data = requests.get(image_url, timeout=10).content\n",
    "                            with open(f\"{output_folder}/img_{downloaded_count}.jpg\", \"wb\") as handler:\n",
    "                                handler.write(img_data)\n",
    "                            downloaded_count += 1\n",
    "\n",
    "                        except Exception as e:\n",
    "                            print(f\"Could not download {image_url}: {e}\")\n",
    "                            continue\n",
    "\n",
    "        # Update offset to fetch the next page of results\n",
    "        params[\"offset\"] += params[\"limit\"]\n",
    "\n",
    "    print(f\"Total images downloaded: {downloaded_count}\")\n",
    "\n",
    "# Run the image download function\n",
    "download_images(species_name, output_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organization of the images as target and non-target (in folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files have been successfully renamed and moved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define paths\n",
    "source_dir = r\"C:\\Users\\sabof\\OneDrive - Université de Lausanne\\Bureau\\UNIL_CEE\\Semester 3\\Machine learning\\dataset\"\n",
    "target_dir = r\"C:\\Users\\sabof\\OneDrive - Université de Lausanne\\Bureau\\UNIL_CEE\\Semester 3\\Machine learning\\dataset_labeled\"\n",
    "\n",
    "# Create target directories if they don't exist\n",
    "os.makedirs(os.path.join(target_dir, 'target'), exist_ok=True)\n",
    "os.makedirs(os.path.join(target_dir, 'non_target'), exist_ok=True)\n",
    "\n",
    "# Define folders containing Amphiprion ocellaris and non-target species\n",
    "amphiprion_folders = [\"Amphiprion_ocellaris_images\"]\n",
    "non_target_folders = [\"Amphiprion_clarkii_images\", \"Neoglyphidodon_oxyodon_images\", \"Neopetrolisthes_maculatus_images\", 'Heteractis_aurora_images' ]\n",
    "\n",
    "# Function to move and rename files\n",
    "def process_folder(folder_list, destination):\n",
    "    for folder in folder_list:\n",
    "        folder_path = os.path.join(source_dir, folder)\n",
    "        for file in os.listdir(folder_path):\n",
    "            if file.endswith((\".jpg\", \".png\")):  # Process only image files\n",
    "                # Create a unique filename by appending the folder name\n",
    "                new_filename = f\"{folder}_{file}\"\n",
    "                source_path = os.path.join(folder_path, file)\n",
    "                destination_path = os.path.join(destination, new_filename)\n",
    "\n",
    "                # Move the file\n",
    "                shutil.move(source_path, destination_path)\n",
    "\n",
    "# Process Amphiprion ocellaris (target)\n",
    "process_folder(amphiprion_folders, os.path.join(target_dir, 'target'))\n",
    "\n",
    "# Process non-target species\n",
    "process_folder(non_target_folders, os.path.join(target_dir, 'non_target'))\n",
    "\n",
    "print(\"Files have been successfully renamed and moved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define paths\n",
    "source_dir = r\"C:\\Users\\sabof\\OneDrive - Université de Lausanne\\Bureau\\UNIL_CEE\\Semester 3\\Machine learning\\dataset\"\n",
    "target_dir = r\"C:\\Users\\sabof\\OneDrive - Université de Lausanne\\Bureau\\UNIL_CEE\\Semester 3\\Machine learning\\dataset_labeled\"\n",
    "for category in ['target', 'non_target']:\n",
    "    count = len(os.listdir(os.path.join(target_dir, category)))\n",
    "    print(f\"{category}: {count} images\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "import random  # For shuffling\n",
    "\n",
    "# Paths\n",
    "labeled_dataset_path = r\"C:\\Users\\sabof\\OneDrive - Université de Lausanne\\Bureau\\UNIL_CEE\\Semester 3\\Machine learning\\dataset_labeled\"\n",
    "output_path = r\"C:\\Users\\sabof\\OneDrive - Université de Lausanne\\Bureau\\UNIL_CEE\\Semester 3\\Machine learning\\processed\"\n",
    "\n",
    "# Function to create train, val, and test splits\n",
    "def create_splits(source_dir, output_dir, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):\n",
    "    assert train_ratio + val_ratio + test_ratio == 1.0, \"Ratios must sum to 1.0\"\n",
    "    random.seed(42)  # Ensure reproducibility\n",
    "\n",
    "    classes = os.listdir(source_dir)  \n",
    "    for cls in classes:\n",
    "        class_path = os.path.join(source_dir, cls)\n",
    "        files = [f for f in os.listdir(class_path) if f.endswith((\".jpg\", \".png\"))]\n",
    "        total_files = len(files)\n",
    "        print(f\"Class '{cls}' has {total_files} images in the labeled dataset.\") \n",
    "\n",
    "        # Shuffle the files to ensure randomness\n",
    "        random.shuffle(files)\n",
    "\n",
    "        # Calculate exact split counts\n",
    "        train_count = math.floor(total_files * train_ratio)\n",
    "        val_count = math.floor(total_files * val_ratio)\n",
    "        test_count = total_files - train_count - val_count  \n",
    "        \n",
    "        # Perform splitting\n",
    "        train_files = files[:train_count]\n",
    "        val_files = files[train_count:train_count + val_count]\n",
    "        test_files = files[train_count + val_count:]\n",
    "        \n",
    "        # Copy files into respective folders\n",
    "        for subset, subset_files in zip([\"train\", \"val\", \"test\"], [train_files, val_files, test_files]):\n",
    "            subset_dir = os.path.join(output_dir, subset, cls)\n",
    "            os.makedirs(subset_dir, exist_ok=True)\n",
    "            for file in subset_files:\n",
    "                source_file = os.path.join(class_path, file)\n",
    "                destination_file = os.path.join(subset_dir, file)\n",
    "                shutil.copy(source_file, destination_file)\n",
    "\n",
    "# Function to count and print images in each folder\n",
    "def count_images_in_folders(output_dir):\n",
    "    for subset in [\"train\", \"val\", \"test\"]:\n",
    "        subset_dir = os.path.join(output_dir, subset)\n",
    "        print(f\"\\n{subset.upper()} FOLDER:\")\n",
    "        total_images = 0\n",
    "        for cls in os.listdir(subset_dir):\n",
    "            class_dir = os.path.join(subset_dir, cls)\n",
    "            num_images = len([f for f in os.listdir(class_dir) if f.endswith((\".jpg\", \".png\"))])\n",
    "            print(f\"  Class '{cls}': {num_images} images\")\n",
    "            total_images += num_images\n",
    "        print(f\"  Total images in {subset}: {total_images}\")\n",
    "\n",
    "# Create splits with labeled dataset\n",
    "create_splits(labeled_dataset_path, output_path)\n",
    "\n",
    "# Count and display images in each folder\n",
    "count_images_in_folders(output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing C:\\Users\\sabof\\OneDrive - Université de Lausanne\\Bureau\\UNIL_CEE\\Semester 3\\Machine learning\\processed\\train\\target:  16%|█▌        | 55/345 [00:02<00:13, 21.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped image C:\\Users\\sabof\\OneDrive - Université de Lausanne\\Bureau\\UNIL_CEE\\Semester 3\\Machine learning\\processed\\train\\target\\Amphiprion_ocellaris_images_img_169.jpg due to incorrect shape: (224, 224, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing C:\\Users\\sabof\\OneDrive - Université de Lausanne\\Bureau\\UNIL_CEE\\Semester 3\\Machine learning\\processed\\train\\target:  28%|██▊       | 96/345 [00:04<00:09, 24.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped image C:\\Users\\sabof\\OneDrive - Université de Lausanne\\Bureau\\UNIL_CEE\\Semester 3\\Machine learning\\processed\\train\\target\\Amphiprion_ocellaris_images_img_218.jpg due to incorrect shape: (224, 224, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing C:\\Users\\sabof\\OneDrive - Université de Lausanne\\Bureau\\UNIL_CEE\\Semester 3\\Machine learning\\processed\\train\\target:  30%|██▉       | 103/345 [00:04<00:09, 25.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped image C:\\Users\\sabof\\OneDrive - Université de Lausanne\\Bureau\\UNIL_CEE\\Semester 3\\Machine learning\\processed\\train\\target\\Amphiprion_ocellaris_images_img_227.jpg due to incorrect shape: (224, 224, 4)\n",
      "Skipped image C:\\Users\\sabof\\OneDrive - Université de Lausanne\\Bureau\\UNIL_CEE\\Semester 3\\Machine learning\\processed\\train\\target\\Amphiprion_ocellaris_images_img_228.jpg due to incorrect shape: (224, 224, 4)\n",
      "Skipped image C:\\Users\\sabof\\OneDrive - Université de Lausanne\\Bureau\\UNIL_CEE\\Semester 3\\Machine learning\\processed\\train\\target\\Amphiprion_ocellaris_images_img_230.jpg due to incorrect shape: (224, 224, 4)\n",
      "Skipped image C:\\Users\\sabof\\OneDrive - Université de Lausanne\\Bureau\\UNIL_CEE\\Semester 3\\Machine learning\\processed\\train\\target\\Amphiprion_ocellaris_images_img_231.jpg due to incorrect shape: (224, 224, 4)\n",
      "Skipped image C:\\Users\\sabof\\OneDrive - Université de Lausanne\\Bureau\\UNIL_CEE\\Semester 3\\Machine learning\\processed\\train\\target\\Amphiprion_ocellaris_images_img_232.jpg due to incorrect shape: (224, 224, 4)\n",
      "Skipped image C:\\Users\\sabof\\OneDrive - Université de Lausanne\\Bureau\\UNIL_CEE\\Semester 3\\Machine learning\\processed\\train\\target\\Amphiprion_ocellaris_images_img_233.jpg due to incorrect shape: (224, 224, 4)\n",
      "Skipped image C:\\Users\\sabof\\OneDrive - Université de Lausanne\\Bureau\\UNIL_CEE\\Semester 3\\Machine learning\\processed\\train\\target\\Amphiprion_ocellaris_images_img_234.jpg due to incorrect shape: (224, 224, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing C:\\Users\\sabof\\OneDrive - Université de Lausanne\\Bureau\\UNIL_CEE\\Semester 3\\Machine learning\\processed\\train\\target:  32%|███▏      | 112/345 [00:04<00:07, 29.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped image C:\\Users\\sabof\\OneDrive - Université de Lausanne\\Bureau\\UNIL_CEE\\Semester 3\\Machine learning\\processed\\train\\target\\Amphiprion_ocellaris_images_img_235.jpg due to incorrect shape: (224, 224, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing C:\\Users\\sabof\\OneDrive - Université de Lausanne\\Bureau\\UNIL_CEE\\Semester 3\\Machine learning\\processed\\train\\target:  45%|████▍     | 154/345 [00:06<00:07, 25.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped image C:\\Users\\sabof\\OneDrive - Université de Lausanne\\Bureau\\UNIL_CEE\\Semester 3\\Machine learning\\processed\\train\\target\\Amphiprion_ocellaris_images_img_29.jpg due to incorrect shape: (224, 224, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing C:\\Users\\sabof\\OneDrive - Université de Lausanne\\Bureau\\UNIL_CEE\\Semester 3\\Machine learning\\processed\\train\\target:  47%|████▋     | 163/345 [00:06<00:06, 26.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped image C:\\Users\\sabof\\OneDrive - Université de Lausanne\\Bureau\\UNIL_CEE\\Semester 3\\Machine learning\\processed\\train\\target\\Amphiprion_ocellaris_images_img_30.jpg due to incorrect shape: (224, 224, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing C:\\Users\\sabof\\OneDrive - Université de Lausanne\\Bureau\\UNIL_CEE\\Semester 3\\Machine learning\\processed\\train\\target:  71%|███████   | 245/345 [00:09<00:03, 27.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped image C:\\Users\\sabof\\OneDrive - Université de Lausanne\\Bureau\\UNIL_CEE\\Semester 3\\Machine learning\\processed\\train\\target\\Amphiprion_ocellaris_images_img_404.jpg due to incorrect shape: (224, 224, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing C:\\Users\\sabof\\OneDrive - Université de Lausanne\\Bureau\\UNIL_CEE\\Semester 3\\Machine learning\\processed\\train\\target:  79%|███████▊  | 271/345 [00:10<00:02, 28.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped image C:\\Users\\sabof\\OneDrive - Université de Lausanne\\Bureau\\UNIL_CEE\\Semester 3\\Machine learning\\processed\\train\\target\\Amphiprion_ocellaris_images_img_436.jpg due to incorrect shape: (224, 224, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing C:\\Users\\sabof\\OneDrive - Université de Lausanne\\Bureau\\UNIL_CEE\\Semester 3\\Machine learning\\processed\\train\\target: 100%|██████████| 345/345 [00:18<00:00, 18.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 331 images with shape (331, 224, 224, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing C:\\Users\\sabof\\OneDrive - Université de Lausanne\\Bureau\\UNIL_CEE\\Semester 3\\Machine learning\\processed\\train\\non_target:  22%|██▏       | 98/444 [00:13<01:00,  5.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped image C:\\Users\\sabof\\OneDrive - Université de Lausanne\\Bureau\\UNIL_CEE\\Semester 3\\Machine learning\\processed\\train\\non_target\\Amphiprion_clarkii_images_img_228.jpg due to incorrect shape: (224, 224, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing C:\\Users\\sabof\\OneDrive - Université de Lausanne\\Bureau\\UNIL_CEE\\Semester 3\\Machine learning\\processed\\train\\non_target:  30%|███       | 134/444 [00:17<00:29, 10.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped image C:\\Users\\sabof\\OneDrive - Université de Lausanne\\Bureau\\UNIL_CEE\\Semester 3\\Machine learning\\processed\\train\\non_target\\Amphiprion_clarkii_images_img_272.jpg due to incorrect shape: (224, 224, 4)\n",
      "Skipped image C:\\Users\\sabof\\OneDrive - Université de Lausanne\\Bureau\\UNIL_CEE\\Semester 3\\Machine learning\\processed\\train\\non_target\\Amphiprion_clarkii_images_img_273.jpg due to incorrect shape: (224, 224, 4)\n",
      "Skipped image C:\\Users\\sabof\\OneDrive - Université de Lausanne\\Bureau\\UNIL_CEE\\Semester 3\\Machine learning\\processed\\train\\non_target\\Amphiprion_clarkii_images_img_274.jpg due to incorrect shape: (224, 224, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing C:\\Users\\sabof\\OneDrive - Université de Lausanne\\Bureau\\UNIL_CEE\\Semester 3\\Machine learning\\processed\\train\\non_target:  96%|█████████▌| 425/444 [00:42<00:01, 10.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped image C:\\Users\\sabof\\OneDrive - Université de Lausanne\\Bureau\\UNIL_CEE\\Semester 3\\Machine learning\\processed\\train\\non_target\\Neopetrolisthes_maculatus_images_img_62.jpg due to incorrect shape: (224, 224, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing C:\\Users\\sabof\\OneDrive - Université de Lausanne\\Bureau\\UNIL_CEE\\Semester 3\\Machine learning\\processed\\train\\non_target: 100%|██████████| 444/444 [00:44<00:00,  9.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 439 images with shape (439, 224, 224, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing C:\\Users\\sabof\\OneDrive - Université de Lausanne\\Bureau\\UNIL_CEE\\Semester 3\\Machine learning\\processed\\val\\target:  35%|███▌      | 26/74 [00:02<00:04, 10.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped image C:\\Users\\sabof\\OneDrive - Université de Lausanne\\Bureau\\UNIL_CEE\\Semester 3\\Machine learning\\processed\\val\\target\\Amphiprion_ocellaris_images_img_219.jpg due to incorrect shape: (224, 224, 4)\n",
      "Skipped image C:\\Users\\sabof\\OneDrive - Université de Lausanne\\Bureau\\UNIL_CEE\\Semester 3\\Machine learning\\processed\\val\\target\\Amphiprion_ocellaris_images_img_229.jpg due to incorrect shape: (224, 224, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing C:\\Users\\sabof\\OneDrive - Université de Lausanne\\Bureau\\UNIL_CEE\\Semester 3\\Machine learning\\processed\\val\\target: 100%|██████████| 74/74 [00:06<00:00, 11.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 72 images with shape (72, 224, 224, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing C:\\Users\\sabof\\OneDrive - Université de Lausanne\\Bureau\\UNIL_CEE\\Semester 3\\Machine learning\\processed\\val\\non_target:   9%|▉         | 9/95 [00:01<00:13,  6.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped image C:\\Users\\sabof\\OneDrive - Université de Lausanne\\Bureau\\UNIL_CEE\\Semester 3\\Machine learning\\processed\\val\\non_target\\Amphiprion_clarkii_images_img_137.jpg due to incorrect shape: (224, 224, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing C:\\Users\\sabof\\OneDrive - Université de Lausanne\\Bureau\\UNIL_CEE\\Semester 3\\Machine learning\\processed\\val\\non_target:  37%|███▋      | 35/95 [00:03<00:04, 12.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped image C:\\Users\\sabof\\OneDrive - Université de Lausanne\\Bureau\\UNIL_CEE\\Semester 3\\Machine learning\\processed\\val\\non_target\\Amphiprion_clarkii_images_img_48.jpg due to incorrect shape: (224, 224, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing C:\\Users\\sabof\\OneDrive - Université de Lausanne\\Bureau\\UNIL_CEE\\Semester 3\\Machine learning\\processed\\val\\non_target: 100%|██████████| 95/95 [00:09<00:00, 10.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 93 images with shape (93, 224, 224, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing C:\\Users\\sabof\\OneDrive - Université de Lausanne\\Bureau\\UNIL_CEE\\Semester 3\\Machine learning\\processed\\test\\target:  21%|██▏       | 16/75 [00:01<00:06,  8.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped image C:\\Users\\sabof\\OneDrive - Université de Lausanne\\Bureau\\UNIL_CEE\\Semester 3\\Machine learning\\processed\\test\\target\\Amphiprion_ocellaris_images_img_226.jpg due to incorrect shape: (224, 224, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing C:\\Users\\sabof\\OneDrive - Université de Lausanne\\Bureau\\UNIL_CEE\\Semester 3\\Machine learning\\processed\\test\\target:  44%|████▍     | 33/75 [00:04<00:07,  5.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped image C:\\Users\\sabof\\OneDrive - Université de Lausanne\\Bureau\\UNIL_CEE\\Semester 3\\Machine learning\\processed\\test\\target\\Amphiprion_ocellaris_images_img_31.jpg due to incorrect shape: (224, 224, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing C:\\Users\\sabof\\OneDrive - Université de Lausanne\\Bureau\\UNIL_CEE\\Semester 3\\Machine learning\\processed\\test\\target: 100%|██████████| 75/75 [00:09<00:00,  8.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 73 images with shape (73, 224, 224, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing C:\\Users\\sabof\\OneDrive - Université de Lausanne\\Bureau\\UNIL_CEE\\Semester 3\\Machine learning\\processed\\test\\non_target:  31%|███▏      | 30/96 [00:03<00:06,  9.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped image C:\\Users\\sabof\\OneDrive - Université de Lausanne\\Bureau\\UNIL_CEE\\Semester 3\\Machine learning\\processed\\test\\non_target\\Amphiprion_clarkii_images_img_275.jpg due to incorrect shape: (224, 224, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing C:\\Users\\sabof\\OneDrive - Université de Lausanne\\Bureau\\UNIL_CEE\\Semester 3\\Machine learning\\processed\\test\\non_target: 100%|██████████| 96/96 [00:12<00:00,  7.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 95 images with shape (95, 224, 224, 3)\n",
      "Train Data shape: (770, 224, 224, 3), Train Labels shape: (770,)\n",
      "Val Data shape: (165, 224, 224, 3), Val Labels shape: (165,)\n",
      "Test Data shape: (168, 224, 224, 3), Test Labels shape: (168,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "# Paths to the split dataset\n",
    "processed_path = r\"C:\\Users\\sabof\\OneDrive - Université de Lausanne\\Bureau\\UNIL_CEE\\Semester 3\\Machine learning\\processed\"\n",
    "train_folder = os.path.join(processed_path, \"train\")\n",
    "val_folder = os.path.join(processed_path, \"val\")\n",
    "test_folder = os.path.join(processed_path, \"test\")\n",
    "\n",
    "# Output dimensions for the images\n",
    "IMG_HEIGHT, IMG_WIDTH = 224, 224\n",
    "\n",
    "# Lists to store data and labels\n",
    "train_data = []\n",
    "train_labels = []\n",
    "val_data = []\n",
    "val_labels = []\n",
    "test_data = []\n",
    "test_labels = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Ensure all images are resized to the same shape and check each one\n",
    "def preprocess_images(folder_path, label):\n",
    "    data = []\n",
    "    labels = []\n",
    "    for file in tqdm(os.listdir(folder_path), desc=f\"Processing {folder_path}\"):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        if file.endswith((\".jpg\", \".png\")):  # Only process image files\n",
    "            try:\n",
    "                # Read the image using Pillow\n",
    "                img = Image.open(file_path)\n",
    "                img_resized = img.resize((224, 224))  # Resize to 224x224\n",
    "                img_normalized = np.array(img_resized) / 255.0  # Normalize\n",
    "\n",
    "                # Check image shape\n",
    "                if img_normalized.shape == (224, 224, 3):  # Check if the image has 3 channels (RGB)\n",
    "                    data.append(img_normalized)\n",
    "                    labels.append(label)\n",
    "                else:\n",
    "                    print(f\"Skipped image {file_path} due to incorrect shape: {img_normalized.shape}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image {file_path}: {e}\")\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    data = np.array(data, dtype=\"float32\")\n",
    "    labels = np.array(labels, dtype=\"int\")\n",
    "    \n",
    "    print(f\"Processed {len(data)} images with shape {data.shape}\")\n",
    "    return data, labels\n",
    "\n",
    "\n",
    "\n",
    "# Preprocess the \"train\", \"val\", and \"test\" data for target and non-target images\n",
    "# Train Data\n",
    "train_target_folder = os.path.join(train_folder, \"target\")\n",
    "train_nontarget_folder = os.path.join(train_folder, \"non_target\")\n",
    "train_target_data, train_target_labels = preprocess_images(train_target_folder, label=1)  # target label is 1\n",
    "train_nontarget_data, train_nontarget_labels = preprocess_images(train_nontarget_folder, label=0)  # non-target label is 0\n",
    "\n",
    "# Validation Data\n",
    "val_target_folder = os.path.join(val_folder, \"target\")\n",
    "val_nontarget_folder = os.path.join(val_folder, \"non_target\")\n",
    "val_target_data, val_target_labels = preprocess_images(val_target_folder, label=1)\n",
    "val_nontarget_data, val_nontarget_labels = preprocess_images(val_nontarget_folder, label=0)\n",
    "\n",
    "# Test Data\n",
    "test_target_folder = os.path.join(test_folder, \"target\")\n",
    "test_nontarget_folder = os.path.join(test_folder, \"non_target\")\n",
    "test_target_data, test_target_labels = preprocess_images(test_target_folder, label=1)\n",
    "test_nontarget_data, test_nontarget_labels = preprocess_images(test_nontarget_folder, label=0)\n",
    "\n",
    "# Combine target and non-target data for each split (train, val, test)\n",
    "train_data = np.concatenate((train_target_data, train_nontarget_data), axis=0)\n",
    "train_labels = np.concatenate((train_target_labels, train_nontarget_labels), axis=0)\n",
    "\n",
    "val_data = np.concatenate((val_target_data, val_nontarget_data), axis=0)\n",
    "val_labels = np.concatenate((val_target_labels, val_nontarget_labels), axis=0)\n",
    "\n",
    "test_data = np.concatenate((test_target_data, test_nontarget_data), axis=0)\n",
    "test_labels = np.concatenate((test_target_labels, test_nontarget_labels), axis=0)\n",
    "\n",
    "# Print the shapes of the datasets\n",
    "print(f\"Train Data shape: {train_data.shape}, Train Labels shape: {train_labels.shape}\")\n",
    "print(f\"Val Data shape: {val_data.shape}, Val Labels shape: {val_labels.shape}\")\n",
    "print(f\"Test Data shape: {test_data.shape}, Test Labels shape: {test_labels.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define the data augmentation generator\n",
    "data_augmentation = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode=\"nearest\"\n",
    ")\n",
    "\n",
    "# Augment the training data\n",
    "augmented_train_data = []\n",
    "augmented_train_labels = []\n",
    "\n",
    "for img, label in tqdm(zip(train_data, train_labels), desc=\"Augmenting Data\"):\n",
    "    img = np.expand_dims(img, axis=0)  \n",
    "    augmented_images = data_augmentation.flow(img, batch_size=1)\n",
    "    for _ in range(5):  # Generate 5 augmented images per original image --> This can be chosen as you wish\n",
    "        augmented_train_data.append(next(augmented_images)[0])  # Use next() to extract the augmented image\n",
    "        augmented_train_labels.append(label)\n",
    "\n",
    "# Convert augmented data to numpy arrays\n",
    "augmented_train_data = np.array(augmented_train_data, dtype=\"float32\")\n",
    "augmented_train_labels = np.array(augmented_train_labels, dtype=\"int\")\n",
    "\n",
    "\n",
    "# Combine the original training data with augmented data\n",
    "train_data_combined = np.concatenate((train_data, augmented_train_data), axis=0)\n",
    "train_labels_combined = np.concatenate((train_labels, augmented_train_labels), axis=0)\n",
    "\n",
    "print(f\"Augmented Train Data shape: {train_data_combined.shape}, Train Labels shape: {train_labels_combined.shape}\")\n",
    "\n",
    "# Use the augmented training data for model training\n",
    "train_data = train_data_combined\n",
    "train_labels = train_labels_combined\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN (custom building)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">222</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">222</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">111</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">111</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">109</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">109</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">86528</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │    <span style=\"color: #00af00; text-decoration-color: #00af00\">11,075,712</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_12 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m222\u001b[0m, \u001b[38;5;34m222\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_12 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m111\u001b[0m, \u001b[38;5;34m111\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_13 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m109\u001b[0m, \u001b[38;5;34m109\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_13 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m54\u001b[0m, \u001b[38;5;34m54\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_14 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m, \u001b[38;5;34m52\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_14 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_4 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m86528\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │    \u001b[38;5;34m11,075,712\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,169,089</span> (42.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m11,169,089\u001b[0m (42.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,169,089</span> (42.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m11,169,089\u001b[0m (42.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# Architecture \n",
    "model = tf.keras.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),  \n",
    "    layers.Dense(1, activation='sigmoid')  \n",
    "])\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), tf.keras.metrics.AUC()])\n",
    "\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Generate a safe log directory for TensorBoard as seen in the course\n",
    "def get_CNN_logdir():\n",
    "    base_dir = os.path.join(os.path.expanduser(\"~\"), \"Documents\", \"CNN_logs\")\n",
    "    time = np.datetime64('now').astype(str).replace(\":\", \"-\")\n",
    "    run_logdir = os.path.abspath(os.path.join(base_dir, f\"run_{time}\"))\n",
    "    os.makedirs(run_logdir, exist_ok=True)\n",
    "    print(f\"Log directory created successfully: {run_logdir}\")\n",
    "    return run_logdir\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Use the augmented training data\n",
    "train_data = train_data_combined\n",
    "train_labels = train_labels_combined\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight(class_weight='balanced',\n",
    "                                     classes=np.unique(train_labels),\n",
    "                                     y=train_labels)\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "\n",
    "print(\"Class Weights:\", class_weights_dict)\n",
    "\n",
    "# Add L2 regularization to Conv2D and Dense layers --> to avoid overfitting\n",
    "for layer in model.layers:\n",
    "    if isinstance(layer, tf.keras.layers.Conv2D) or isinstance(layer, tf.keras.layers.Dense):\n",
    "        layer.kernel_regularizer = l2(0.01)\n",
    "\n",
    "# Recompile the model after adding regularization\n",
    "model.compile(optimizer='adam', \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), tf.keras.metrics.AUC()])\n",
    "\n",
    "# Set up Early Stopping and Learning Rate Scheduler\n",
    "early_stopping_cb = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "reduce_lr_cb = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-5)\n",
    "\n",
    "# Train the model with class weights, early stopping, and learning rate scheduler\n",
    "try:\n",
    "    history = model.fit(\n",
    "        train_data, train_labels,\n",
    "        epochs=30,\n",
    "        batch_size=32,\n",
    "        validation_data=(val_data, val_labels),\n",
    "        class_weight=class_weights_dict,\n",
    "        callbacks=[early_stopping_cb, reduce_lr_cb],\n",
    "        verbose=2\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Displays the first predictions to verify the model's binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for the test data\n",
    "predictions = model.predict(test_data)\n",
    "\n",
    "# Example: the first 5 predictions\n",
    "for i in range(5):\n",
    "    print(f\"Prediction: {predictions[i]}, True Label: {test_labels[i]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import metrics \n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "evaluation_results = model.evaluate(test_data, test_labels, verbose=2)\n",
    "\n",
    "# Print the results\n",
    "metrics_names = model.metrics_names\n",
    "for name, value in zip(metrics_names, evaluation_results):\n",
    "    print(f\"{name}: {value}\")\n",
    "\n",
    "\n",
    "\n",
    "# Calculate precision, recall.\n",
    "predictions = (model.predict(test_data) > 0.5).astype(int)\n",
    "precision = precision_score(test_labels, predictions, zero_division=1)\n",
    "recall = recall_score(test_labels, predictions, zero_division=1)\n",
    "f1 = f1_score(test_labels, predictions, zero_division=1)\n",
    "\n",
    "\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss over epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy over epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Get the prediction probabilities\n",
    "prediction_probs = model.predict(test_data)\n",
    "\n",
    "# Calculate the false positive rate, true positive rate, and thresholds\n",
    "fpr, tpr, _ = roc_curve(test_labels, prediction_probs)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot the AUC-ROC curve\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(fpr, tpr, color='b', lw=2, label=f'AUC = {roc_auc:.2f}')\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir=C:/Users/sabof/Documents/CNN_logs --port=6006 #feel free to use another port if needed i tried 6007 but was satured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = model.predict(test_data)\n",
    "\n",
    "# Convert predictions to binary classes (0 or 1) treshold set at 0.5\n",
    "predicted_classes = (predictions > 0.5).astype(int).flatten()\n",
    "\n",
    "# Define true labels \n",
    "true_classes = test_labels  # Labels should be 0 or 1 directly\n",
    "\n",
    "# Define class names\n",
    "class_names = [\"Class 0\", \"Class 1\"]\n",
    "\n",
    "# Generate confusion matrix\n",
    "conf_matrix = confusion_matrix(true_classes, predicted_classes)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "# Generate a classification report\n",
    "report = classification_report(true_classes, predicted_classes, target_names=class_names)\n",
    "print(\"\\nClassification Report:\\n\", report)\n",
    "\n",
    "\n",
    "# Generate predictions\n",
    "predictions = (model.predict(test_data) > 0.5).astype(int)\n",
    "\n",
    "# Generate confusion matrix\n",
    "conf_matrix = confusion_matrix(test_labels, predictions)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='g', xticklabels=['Non-Target', 'Target'], yticklabels=['Non-Target', 'Target'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To look at the imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the distribution of training and validation labels\n",
    "unique, counts = np.unique(train_labels, return_counts=True)\n",
    "print(\"Training set distribution:\", {int(k): int(v) for k, v in zip(unique, counts)})\n",
    "\n",
    "unique, counts = np.unique(val_labels, return_counts=True)\n",
    "print(\"Validation set distribution:\", {int(k): int(v) for k, v in zip(unique, counts)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XAI saliency map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Modify to plot all images in the same figure\n",
    "fig, axes = plt.subplots(4, 3, figsize=(15, 20))  # 4 rows for each image, 3 columns for original, saliency, and superimposed\n",
    "\n",
    "for idx, i in enumerate(range(79, 83)):  # Adjust range to match 4 images\n",
    "    sample_image = tf.convert_to_tensor(test_data[i:i+1], dtype=tf.float32)  # Convert to tensor with dtype float32\n",
    "\n",
    "    # Get the gradients of the loss with respect to the input image\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(sample_image)\n",
    "        prediction = model(sample_image)\n",
    "        loss = prediction[0]  # Get the predicted score for the positive class\n",
    "\n",
    "    gradients = tape.gradient(loss, sample_image)\n",
    "\n",
    "    # Take the maximum along the color channels\n",
    "    saliency = np.max(np.abs(gradients), axis=-1)[0]\n",
    "\n",
    "    # Normalize the saliency map to be in the range [0, 1]\n",
    "    saliency = (saliency - saliency.min()) / (saliency.max() - saliency.min() + 1e-10)\n",
    "\n",
    "    # Enhance brightness of the saliency map\n",
    "    saliency = np.clip(saliency * 1.5, 0, 1)  # Scale values for increased brightness\n",
    "\n",
    "    # Create a superimposed image by blending the saliency map with the original image\n",
    "    saliency_resized = cv2.resize(saliency, (sample_image.shape[2], sample_image.shape[1]))\n",
    "    saliency_colored = cv2.applyColorMap((saliency_resized * 255).astype(np.uint8), cv2.COLORMAP_JET)\n",
    "    original_image = (sample_image[0].numpy() * 255).astype(np.uint8)  # Ensure correct scaling and data type\n",
    "    superimposed_image = cv2.addWeighted(original_image, 0.6, saliency_colored, 0.4, 0)\n",
    "\n",
    "    # Plot the original image, saliency map, and superimposed image in the same figure\n",
    "    axes[idx, 0].imshow(original_image)\n",
    "    axes[idx, 0].axis('off')\n",
    "    axes[idx, 0].set_title(f'Original Image {i+1}')\n",
    "\n",
    "    axes[idx, 1].imshow(saliency, cmap='hot')\n",
    "    axes[idx, 1].axis('off')\n",
    "    axes[idx, 1].set_title(f'Saliency Map {i+1}')\n",
    "\n",
    "    axes[idx, 2].imshow(superimposed_image)\n",
    "    axes[idx, 2].axis('off')\n",
    "    axes[idx, 2].set_title(f'Superimposed Image {i+1}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "# Path to the special set of images\n",
    "special_set_dir = r\"C:\\Users\\sabof\\OneDrive - Université de Lausanne\\Bureau\\try_detect\"\n",
    "\n",
    "# List all images in the directory\n",
    "image_paths = [os.path.join(special_set_dir, fname) for fname in os.listdir(special_set_dir) if fname.lower().endswith(('png', 'jpg', 'jpeg'))]\n",
    "\n",
    "# Perform predictions and print the results\n",
    "for image_path in image_paths:\n",
    "    # Load and preprocess the image\n",
    "    img = load_img(image_path, target_size=(224, 224))\n",
    "    sample_image = img_to_array(img) / 255.0\n",
    "    sample_image_tensor = tf.convert_to_tensor(sample_image[None, ...], dtype=tf.float32)  # Add batch dimension\n",
    "\n",
    "    # Get the prediction\n",
    "    prediction = model(sample_image_tensor)\n",
    "    predicted_label = int(prediction[0] > 0.5)  \n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Image: {os.path.basename(image_path)} - Predicted Label: {predicted_label} - Raw Prediction: {prediction[0].numpy()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saliency map on a common dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "# Path to the special set of images\n",
    "special_set_dir = r\"C:\\Users\\sabof\\OneDrive - Université de Lausanne\\Bureau\\try_detect\"\n",
    "\n",
    "# List all images in the directory\n",
    "image_paths = [os.path.join(special_set_dir, fname) for fname in os.listdir(special_set_dir) if fname.lower().endswith(('png', 'jpg', 'jpeg'))]\n",
    "\n",
    "# Prepare figure for plotting\n",
    "fig, axes = plt.subplots(len(image_paths), 3, figsize=(15, len(image_paths) * 5))\n",
    "\n",
    "for idx, image_path in enumerate(image_paths):\n",
    "    # Load and preprocess the image\n",
    "    img = load_img(image_path, target_size=(224, 224))\n",
    "    sample_image = img_to_array(img) / 255.0\n",
    "    sample_image_tensor = tf.convert_to_tensor(sample_image[None, ...], dtype=tf.float32)  # Add batch dimension\n",
    "\n",
    "    # Get the gradients of the loss with respect to the input image\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(sample_image_tensor)\n",
    "        prediction = model(sample_image_tensor)  # Recompute prediction within the tape\n",
    "        loss = tf.reduce_max(prediction)\n",
    "        predicted_label = int(prediction[0].numpy() > 0.5)  # Use the predicted score for the top class\n",
    "\n",
    "    gradients = tape.gradient(loss, sample_image_tensor)\n",
    "\n",
    "    # Check if gradients are valid\n",
    "    if gradients is None:\n",
    "        print(f\"Gradients are None for image {os.path.basename(image_path)}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Take the maximum along the color channels\n",
    "    saliency = np.max(np.abs(gradients), axis=-1)[0]\n",
    "\n",
    "    # Normalize the saliency map to be in the range [0, 1]\n",
    "    saliency = (saliency - saliency.min()) / (saliency.max() - saliency.min() + 1e-10)\n",
    "\n",
    "    # Enhance brightness of the saliency map\n",
    "    saliency = np.clip(saliency * 1.5, 0, 1)  # Scale values for increased brightness\n",
    "\n",
    "    # Create a superimposed image by blending the saliency map with the original image\n",
    "    saliency_resized = cv2.resize(saliency, (sample_image.shape[1], sample_image.shape[0]))\n",
    "    saliency_colored = cv2.applyColorMap((saliency_resized * 255).astype(np.uint8), cv2.COLORMAP_JET)\n",
    "    original_image = (sample_image * 255).astype(np.uint8)  # Ensure correct scaling and data type\n",
    "    superimposed_image = cv2.addWeighted(original_image, 0.6, saliency_colored, 0.4, 0)\n",
    "\n",
    "    # Plot the original image, saliency map, and superimposed image\n",
    "    axes[idx, 0].imshow(original_image)\n",
    "    axes[idx, 0].axis('off')\n",
    "    axes[idx, 0].set_title(f\"Original Image Predicted: {predicted_label}\", fontsize=10)\n",
    "\n",
    "    axes[idx, 1].imshow(saliency, cmap='hot')\n",
    "    axes[idx, 1].axis('off')\n",
    "    axes[idx, 1].set_title('Saliency Map')\n",
    "\n",
    "    axes[idx, 2].imshow(superimposed_image)\n",
    "    axes[idx, 2].axis('off')\n",
    "    axes[idx, 2].set_title('Superimposed Image')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
